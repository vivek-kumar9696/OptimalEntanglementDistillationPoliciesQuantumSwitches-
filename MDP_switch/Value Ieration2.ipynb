{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff8d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "float(transition_probability[data['Initial State'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ab41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data from the xlsx file\n",
    "data = pd.read_excel('Final_transition_mStar_2_q_0.6_plink_0.5.xlsx')\n",
    "\n",
    "# Initialize the parameters\n",
    "gamma = 0.9\n",
    "epsilon = 0.001\n",
    "\n",
    "# Initialize the state space\n",
    "states = data['Initial State'].unique()\n",
    "\n",
    "# Initialize the action space\n",
    "actions = data['Action'].unique()\n",
    "\n",
    "# Initialize the transition probability matrix\n",
    "transition_probability = np.zeros((len(states), len(actions), len(states)))\n",
    "\n",
    "# Initialize the reward matrix\n",
    "reward = np.zeros((len(states), len(actions), len(states)))\n",
    "\n",
    "# Initialize the value function\n",
    "value_function = np.zeros(len(states))\n",
    "\n",
    "# Initialize the policy\n",
    "policy = np.zeros(len(states))\n",
    "\n",
    "# Initialize the policy improvement flag\n",
    "policy_stable = False\n",
    "\n",
    "# Initialize the iteration counter\n",
    "iteration = 0\n",
    "\n",
    "# Initialize the reward matrix\n",
    "for i in range(len(data)):\n",
    "    transition_probability[data['Initial State'][i] - 1, data['Action'][i] - 1, data['Final State'][i] - 1] = data['Transition Probability'][i]\n",
    "    reward[data['Initial State'][i] - 1, data['Action'][i] - 1, data['Final State'][i] - 1] = data['Reward'][i]\n",
    "\n",
    "# Value iteration algorithm\n",
    "while not policy_stable:\n",
    "    policy_stable = True\n",
    "    delta = 0\n",
    "    for s in states:\n",
    "        old_value = value_function[s - 1]\n",
    "        action_values = np.zeros(len(actions))\n",
    "        for a in actions:\n",
    "            action_values[a - 1] = np.sum(transition_probability[s - 1, a - 1, :] * (reward[s - 1, a - 1, :] + gamma * value_function))\n",
    "        value_function[s - 1] = np.max(action_values)\n",
    "        delta = max(delta, np.abs(old_value - value_function[s - 1]))\n",
    "        old_policy = policy[s - 1]\n",
    "        policy[s - 1] = np.argmax(action_values) + 1\n",
    "        if old_policy != policy[s - 1]:\n",
    "            policy_stable = False\n",
    "    iteration += 1\n",
    "    if delta < epsilon:\n",
    "        break\n",
    "\n",
    "# Print the optimal policy\n",
    "print('The optimal policy is:')\n",
    "print(policy)\n",
    "\n",
    "# Print the optimal value function\n",
    "print('The optimal value function is:')\n",
    "print(value_function)\n",
    "\n",
    "# Print the number of iterations\n",
    "print('The number of iterations is:')\n",
    "print(iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "def value_iteration(df, gamma, theta):\n",
    "    # Initialize value function and policy with zeros\n",
    "    n_states = df[\"Initial State\"].nunique()\n",
    "    value_function = [0] * n_states\n",
    "    policy = [0] * n_states\n",
    "\n",
    "    # Initialize utility values for each action\n",
    "    action_values = {\n",
    "        action: [0] * n_states for action in df[\"Action\"].unique()\n",
    "    }\n",
    "\n",
    "    # Iterate until convergence\n",
    "    while True:\n",
    "        value_function_prev = copy.deepcopy(value_function)\n",
    "        # Compute new value function\n",
    "        for i in range(n_states):\n",
    "            for action, action_value in action_values.items():\n",
    "                # Compute the expected return for this action\n",
    "                action_value[i] = sum(\n",
    "                    df[(df[\"Initial State\"] == i) & (df[\"Action\"] == action)]\n",
    "                    .apply(\n",
    "                        lambda row: (\n",
    "                            row[\"Transition Probability\"]\n",
    "                            * (row[\"Reward\"] + gamma * value_function[row[\"Final State\"]])\n",
    "                        ),\n",
    "                        axis=1,\n",
    "                    )\n",
    "                    .values\n",
    "                )\n",
    "\n",
    "            # Select the best action and update the value function\n",
    "            best_action = max(action_values, key=lambda action: action_values[action][i])\n",
    "            policy[i] = best_action\n",
    "            value_function[i] = action_values[best_action][i]\n",
    "\n",
    "        # Check for convergence\n",
    "        if all(abs(value_function[i] - value_function_prev[i]) < theta for i in range(n_states)):\n",
    "            break\n",
    "\n",
    "    return policy\n",
    "\n",
    "# Read data from Excel file\n",
    "df = pd.read_excel(\"Final_transition_mStar_2_q_0.6_plink_0.5.xlsx\")\n",
    "\n",
    "# Run value iteration\n",
    "policy = value_iteration(df, gamma=0.9, theta=1e-3)\n",
    "\n",
    "# Print the optimal policy\n",
    "print(\"Optimal policy:\", policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"Final_transition_mStar_2_q_0.6_plink_0.5.xlsx\")\n",
    "rewards = {}\n",
    "transition_probs = {}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    initial_state = row[\"Initial State\"]\n",
    "    final_state = row[\"Final State\"]\n",
    "    action = row[\"Action\"]\n",
    "    reward = row[\"Reward\"]\n",
    "    transition_prob = row[\"Transition Probability\"]\n",
    "    # Add the reward to the rewards dictionary\n",
    "    rewards[initial_state] = reward\n",
    "\n",
    "# Add the transition probability to the transition probabilities dictionary\n",
    "    if initial_state not in transition_probs:\n",
    "        transition_probs[initial_state] = {}\n",
    "    if action not in transition_probs[initial_state]:\n",
    "        transition_probs[initial_state][action] = {}\n",
    "        transition_probs[initial_state][action][final_state] = transition_prob\n",
    "        \n",
    "        \n",
    "discount_factor = 0.9\n",
    "max_error = 0.001\n",
    "state_values = {state: 0 for state in rewards.keys()}\n",
    "optimal_policy = {state: None for state in rewards.keys()}\n",
    "\n",
    "while True:\n",
    "    # Initialize the maximum change in state value to 0\n",
    "    max_change = 0.001\n",
    "    # Loop through all states\n",
    "    for state in rewards.keys():\n",
    "        # Initialize the value of the state to 0\n",
    "        state_value = 0\n",
    "    \n",
    "        # Loop through all possible actions from the state\n",
    "        for action in transition_probs[state].keys():\n",
    "            # Initialize the expected value of the action to 0\n",
    "            action_value = 0\n",
    "    \n",
    "            # Loop through all possible final states from the action\n",
    "            for final_state in transition_probs[state][action].keys():\n",
    "                # Calculate the expected value of the action\n",
    "                action_value += transition_probs[state][action][final_state] * state_values[final_state]\n",
    "    \n",
    "            # Calculate the value of the state\n",
    "            state_value = max(state_value, rewards[state] + discount_factor * action_value)\n",
    "    \n",
    "        # Calculate the change in state value\n",
    "        change = abs(state_values[state] - state_value)\n",
    "        max_change = max(max_change, change)\n",
    "    \n",
    "        # Update the state value and the optimal policy\n",
    "        state_values[state] = state_value\n",
    "        for action in transition_probs[state].keys():\n",
    "            action_value = 0\n",
    "            for final_state in transition_probs[state][action].keys():\n",
    "                action_value += transition_probs[state][action][final_state] * state_values[final_state]\n",
    "            if action_value > state_value:\n",
    "                optimal_policy[state] = action\n",
    "    \n",
    "    # Check if the maximum change is less than the maximum error allowed\n",
    "    print(max_change, max_error)\n",
    "    if max_change <= max_error:\n",
    "        break\n",
    "\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be1b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import copy as cp\n",
    "data = pd.read_excel('Final_transition_mStar_3_q_0.6_plink_0.7.xlsx')\n",
    "\n",
    "delta = 0\n",
    "discount = 0.9\n",
    "values = {}\n",
    "optimal_policy = {}\n",
    "\n",
    "for initial_state in data['Initial State'].unique():\n",
    "    values[initial_state] = 0\n",
    "\n",
    "while delta < 10:\n",
    "    for initial_state in data['Initial State'].unique():\n",
    "        max_value = cp.deepcopy(values[initial_state])\n",
    "        for action in data[data['Initial State'] == initial_state]['Action Taken'].unique():\n",
    "            print(initial_state,action)\n",
    "            # initialize the expected reward to be 0\n",
    "            expected_reward = 0\n",
    "            # iterate over all final states\n",
    "            for final_state in data[(data['Initial State'] == initial_state) & (data['Action Taken'] == action)]['Final State'].unique():\n",
    "                # calculate the expected reward by summing over all final states\n",
    "                print(\"Final State\", final_state)\n",
    "                expected_reward += data[(data['Initial State'] == initial_state) & (data['Action Taken'] == action) & (data['Final State'] == final_state)]['Expected Reward'].values[0] * data[(data['Initial State'] == initial_state) & (data['Action Taken'] == action) & (data['Final State'] == final_state)]['Transition Probability'].values[0]\n",
    "                # calculate the value of taking action in the initial state\n",
    "                value = expected_reward + discount * values[final_state]\n",
    "                # update the max value and the optimal action if necessary\n",
    "                if value > max_value:\n",
    "                    max_value = cp.deepcopy(value)\n",
    "                    optimal_policy[initial_state] = action\n",
    "                    values[initial_state] = cp.deepcopy(max_value)\n",
    "    delta = delta + 1 \n",
    "    print()\n",
    "                \n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34707a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "fid_thresh = 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73571d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Final_Dis2_sep_ES_fid_threshFinal_0.9both_queues_transition_mStar_3queue_length=3_q_1_plink_0.7alpha_0.01.xlsx')\n",
    "    \n",
    "# Initialize the parameters\n",
    "gamma = 0.9\n",
    "delta = 0\n",
    "\n",
    "# Initialize the state space\n",
    "states = data['Initial State'].unique()\n",
    "\n",
    "\n",
    "# Initialize the action space\n",
    "actions = data['Action'].unique()\n",
    "\n",
    "V = {state:0 for state in states}\n",
    "V_old = {state:-1 for state in states}\n",
    "policy = {state:'None' for state in states}\n",
    "\n",
    "fin_policy = []\n",
    "while delta < 100:\n",
    "    V_old = V.copy()\n",
    "    for s in states:\n",
    "        \n",
    "        q = {str(action):0 for action in data[(data['Initial State'] == s)][\"Action\"]}\n",
    "        \n",
    "        for a in data[(data['Initial State'] == s)][\"Action\"].unique():\n",
    "            \n",
    "            act_df = data[(data['Initial State'] == s)][(data['Action'] == a)]\n",
    "            act_df = act_df.reset_index()\n",
    "            for index, row in act_df.iterrows():\n",
    "                q[str(row[\"Action\"])] += row['Transition Probability']*(float(row[\"Reward\"])+gamma*V_old[row[\"Final State\"]])\n",
    "        max_q_key = max(q, key=q.get)\n",
    "        optimum_action = max_q_key\n",
    "        V[s] = q[max_q_key]\n",
    "        policy[s] = [optimum_action, q[max_q_key]]\n",
    "    delta = delta + 1 \n",
    "    \n",
    "for key, value in policy.items():\n",
    "    func = (data[(data['Initial State'] == key) & (data['Action'] == value[0])][\"function Name\"].iloc[0])\n",
    "    fin_policy.append([key,value[0], func])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa46a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(fin_policy, columns=['State', 'Action', 'Func'])\n",
    "\n",
    "output = \"ES_Distill_ind_Policy_mstar_3_fidThresh\"+str(fid_thresh) + \"alpha_0_01.xlsx\"\n",
    "df.to_excel(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c07273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "while fid_thresh <= 1:\n",
    "    data = pd.read_excel('Final_Dis2_sep_ES_fid_threshFinal_'+str(fid_thresh)+'both_queues_transition_mStar_3queue_length=3_q_1_plink_0.7alpha_0.23104906018664842.xlsx')\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    gamma = 0.9\n",
    "    delta = 0\n",
    "    \n",
    "    # Initialize the state space\n",
    "    states = data['Initial State'].unique()\n",
    "    \n",
    "    \n",
    "    # Initialize the action space\n",
    "    actions = data['Action'].unique()\n",
    "    \n",
    "    V = {state:0 for state in states}\n",
    "    V_old = {state:-1 for state in states}\n",
    "    policy = {state:'None' for state in states}\n",
    "    \n",
    "    fin_policy = []\n",
    "    while delta < 100:\n",
    "        V_old = V.copy()\n",
    "        for s in states:\n",
    "            \n",
    "            q = {str(action):0 for action in data[(data['Initial State'] == s)][\"Action\"]}\n",
    "            \n",
    "            for a in data[(data['Initial State'] == s)][\"Action\"].unique():\n",
    "                \n",
    "                act_df = data[(data['Initial State'] == s)][(data['Action'] == a)]\n",
    "                act_df = act_df.reset_index()\n",
    "                for index, row in act_df.iterrows():\n",
    "                    q[str(row[\"Action\"])] += row['Transition Probability']*(float(row[\"Reward\"])+gamma*V_old[row[\"Final State\"]])\n",
    "            max_q_key = max(q, key=q.get)\n",
    "            optimum_action = max_q_key\n",
    "            V[s] = q[max_q_key]\n",
    "            policy[s] = [optimum_action, q[max_q_key]]\n",
    "        delta = delta + 1 \n",
    "        \n",
    "    for key, value in policy.items():\n",
    "        func = (data[(data['Initial State'] == key) & (data['Action'] == value[0])][\"function Name\"].iloc[0])\n",
    "        fin_policy.append([key,value[0], func])\n",
    "        \n",
    "    df = pd.DataFrame(fin_policy, columns=['State', 'Action', 'Func'])\n",
    "    \n",
    "    output = \"ES_Distill_ind_Policy_mstar_3_fidThresh\"+str(fid_thresh) + \".xlsx\"\n",
    "    df.to_excel(output)\n",
    "    fid_thresh += 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a541ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70933d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a6209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47553a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be299d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181d620",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
